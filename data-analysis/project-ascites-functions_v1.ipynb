{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca697c-de7a-40b9-8e90-436f1b5035e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: Ascites\n",
    "# content: Functions for data analysis\n",
    "\r",
    "# author: kraesing\n",
    "# mail: lau.kraesing.vestergaard@regionh.dk\n",
    "# GitHub: https://github.com/kraesing\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a179121-9398-4188-8764-bb6f18741b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT LIBRARIES\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Libraries for file management.\n",
    "import os\n",
    "import glob \n",
    "import fnmatch\n",
    "import shutil \n",
    "\n",
    "# Libraries for data management.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for statistical analysis.\n",
    "import scipy.stats as stats\n",
    "import scipy.spatial.distance import squareform, pdist\n",
    "\n",
    "# Libraries for plotting.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Libraries for machine learning.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Libraries for Natural Language Processing (NLP)\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Libraries for Network analysis\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Conformation\n",
    "print(\"Libraries imported!\")\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501d9ba-836e-4dfc-893b-88f2b82c5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining variables \n",
    "\n",
    "path = \"H:/PhD/Work_Packages/Work_package1_Ascites/Data/\"\n",
    "path_to_files = \"/ClinVar_dir/output_files_annotated/Overlapping files/\"\n",
    "path_to_clinical_data = r\"H:\\PhD\\Work_Packages\\Work_package1_Ascites\\Kliniske_data\\DGCD_data_sampling_updated.xlsx\"\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b638e-0e58-4541-b036-e039c2cdfc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change Directory\n",
    "\n",
    "os.chdir(path + \"IonTorrent5.18\")\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8c2e9-5adb-495a-bb33-2e5ff1e3343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up environment for figures\n",
    "\n",
    "if os.path.exists(\"figures\"):\n",
    "    print(\"Directory for figures already exist!\")\n",
    "else:\n",
    "    os.mkdir(\"figures\")\n",
    "    print(\"Directory for figures has been created!b\")\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe6703-5618-4717-88c6-12e9f509ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for saving figures\n",
    "\n",
    "def save_fig(plot_name ,filename, transparent_background):\n",
    "    os.chdir(path + \"figures\")\n",
    "    plot_name.savefig(filename, format=\"svg\", dpi=600, bbox_inches='tight', \n",
    "                      transparent=transparent_background)\n",
    "    print(f\"Your figure: {filename}, has been saved!\")\n",
    "    os.chdir(path)\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad60c3-7a48-4111-a03c-2d2aee423e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for counting initial variants\n",
    "\n",
    "def counting_variants(input_list1, input_list2):\n",
    "    variant_counts1 = []\n",
    "    for i in input_list1:\n",
    "        variant_counts1.append(pd.read_csv(i, delimiter=\"\\t\").assign(biopsy_type=\"ascites\"))\n",
    "    variant_counts2 = []\n",
    "    for i in input_list2:\n",
    "        variant_counts2.append(pd.read_csv(i, delimiter=\"\\t\").assign(biopsy_type=\"tissue\"))\n",
    "    \n",
    "    df_combined = pd.concat(variant_counts1 + variant_counts2)\n",
    "    print(\"total number of variants:\", len(df_combined))\n",
    "    print(df_combined.biopsy_type.value_counts())\n",
    "    print(df_combined.Genes.nunique())\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61cc11-a103-41ec-a16c-89330c3f011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for initial variant filtering\n",
    "\n",
    "### Create empty lists\n",
    "df_list = []\n",
    "list_nocall = []\n",
    "df_filtered = []\n",
    "\n",
    "def Initial_cleaning(df):\n",
    "    List_of_types = [\"SNV\", \"INDEL\", \"MNV\"]\n",
    "    list_nocall.append(df.loc[df[\"Filter\"] == \"NOCALL\"])\n",
    "    df = df.loc[df[\"Type\"].isin(List_of_types)]\n",
    "    df.dropna(how=\"all\", axis=1, inplace=True)\n",
    "    list_to_remove = []\n",
    "    for col in df.columns:\n",
    "        if \"ExAC\" in col:\n",
    "            list_to_remove.append(col)\n",
    "    df = df.drop(columns=list_to_remove)\n",
    "    #df = df.drop(columns=([\"SIFT\", \"Grantham\", \"PolyPhen\", \"PhyloP\", \"DrugBank\"]))\n",
    "    df[\"Chrom\"] = df[\"Locus\"].str.split(\":\").str[0]\n",
    "    df[\"Start\"] = pd.to_numeric(df[\"Locus\"].str.split(\":\").str[1])\n",
    "    df[\"End\"] = df[\"Start\"] + df[\"Length\"] - 1\n",
    "    df[\"Base_call_accuracy\"] = 1-10**-(df[\"Phred QUAL Score\"]/10)\n",
    "    list_of_allele_ratios = list(df[\"Allele Ratio\"].str.findall(\"\\d+\\.?\\d+\"))\n",
    "    df[[\"Allele_Ratio1\", \"Allele_Ratio2\"]] = list_of_allele_ratios\n",
    "    df[[\"Allele_Ratio1\", \"Allele_Ratio2\"]] = df[[\"Allele_Ratio1\", \"Allele_Ratio2\"]].apply(pd.to_numeric, axis=1)\n",
    "    df[\"Potential_germline\"] = df[\"Allele_Ratio2\"].apply(lambda x: \"Potential germline variant\" if x >= 0.98 else \"NaN\")\n",
    "    df = df.loc[df[\"Potential_germline\"] != \"Potential germline variant\"]\n",
    "    df = df.loc[df[\"UCSC Common SNPs\"] != \"YES\"]\n",
    "    df = df.loc[df[\"Variant Effect\"] != \"synonymous\"]\n",
    "    df = df.loc[(df[\"Location\"].str.contains(\"exonic\", na=False)) | (df[\"Location\"].str.contains(\"splicesite\", na=False))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf5fc4-83fd-4b6c-98e9-191fd07f0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for filtering genes\n",
    "# Based on previous studies from: Vestergaard et al., 2021 - doi: 10.3390/cancers13205230. PMID: 34680378; PMCID: PMC8533843.\n",
    "# Genes are filtered on overlapping genes and nucleotide positions.\n",
    "# OCAv3: Ascites samples, OCA-Plus: Tumor Tissue Samples\n",
    "\n",
    "def gene_cleaning():\n",
    "    df_OCA_bed = pd.read_csv(r\"OCAv3.20180509.Designed.bed\", sep=\"\\t\", skiprows=(1), header=None)\n",
    "    df_OCA_bed = df_OCA_bed.set_axis([\"Chr\", \"Start\", \"End\", \"primer_ID\", \"seperator\", \"Gene_information\"], axis=1)\n",
    "\n",
    "    df_OCAPlus_bed = pd.read_csv(r\"OCAPlus.20191203.designed.bed\", sep=\"\\t\", skiprows=(1), header=None)\n",
    "    df_OCAPlus_bed = df_OCAPlus_bed.set_axis([\"Chr\", \"Start\", \"End\", \"primer_ID\", \"seperator\", \"Gene_information\"], axis=1)\n",
    "\n",
    "    df_OCA_bed[\"Genes\"] = df_OCA_bed[\"Gene_information\"].str.split(\";\").str[0].str.split(\"=\").str[1]\n",
    "    OCA_genes = set(df_OCA_bed[\"Genes\"].unique().tolist())\n",
    "\n",
    "    df_OCAPlus_bed[\"Genes\"] = df_OCAPlus_bed[\"Gene_information\"].str.split(\";\").str[0].str.split(\"=\").str[1]\n",
    "    df_OCAPlus_bed = df_OCAPlus_bed[~df_OCAPlus_bed.Genes.str.contains(\"rs\")]\n",
    "    OCAPlus_genes = set(df_OCAPlus_bed[\"Genes\"].unique().tolist())\n",
    "\n",
    "    Genes_int = set(OCA_genes.intersection(OCAPlus_genes))\n",
    "    Unique_genes_OCA = Genes_int.symmetric_difference(OCA_genes)\n",
    "    Unique_genes_OCAPlus = OCAPlus_genes.symmetric_difference(Genes_int)\n",
    "\n",
    "    OCP_Start_end = df_OCA_bed[[\"Start\", \"End\", \"Genes\"]]\n",
    "    OCPPlus_Start_end = df_OCAPlus_bed[[\"Start\", \"End\", \"Genes\"]]\n",
    "\n",
    "    OCP_Start_end[\"Covered_positions\"] = OCP_Start_end.apply(lambda x : list(range(x[\"Start\"], x[\"End\"]+1)),1)\n",
    "    OCPPlus_Start_end[\"Covered_positions\"] = OCPPlus_Start_end.apply(lambda x : list(range(x[\"Start\"], x[\"End\"]+1)),1)\n",
    "\n",
    "    OCP_Start_end1 = OCP_Start_end.explode(\"Covered_positions\")\n",
    "    OCPPlus_Start_end1 = OCPPlus_Start_end.explode(\"Covered_positions\")\n",
    "\n",
    "    int_pos = set(set(OCP_Start_end1[\"Covered_positions\"]).intersection(set(OCPPlus_Start_end1[\"Covered_positions\"])))\n",
    "    \n",
    "    return Genes_int, int_pos\n",
    "\n",
    "genes_int, int_pos = gene_cleaning()\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347f33e-d138-4cc4-af53-609d4139fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions for additional gene filtering. \n",
    "\n",
    "def Gene_cleaning(df):\n",
    "    df = df.assign(Genes1=df[\"Genes\"].str.split(\",\")).explode(\"Genes1\")\n",
    "    df = df.loc[(df[\"Genes1\"].isin(genes_int)) & (df[\"Start\"].isin(int_pos))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ae123-23aa-4f8c-8def-f5d89a49fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for original filtering of variants\n",
    "\n",
    "def Original_filtering(df, allele_freq, coverage):  \n",
    "    df.loc[df[\"Allele Frequency %\"] < allele_freq, \"true_variant\"] = \"Below threshold for allele frequency\"\n",
    "    df.loc[df[\"Coverage\"] < (coverage), \"true_variant\"] = \"Low overall coverage (caution warrant)\"                                                                  \n",
    "    df.loc[(df[\"Homopolymer Length\"] >= 5), \"true_variant\"] = \"High homopolymer content\"                                          \n",
    "    m = df.loc[lambda x: x[\"Coverage\"] >= 100]                                                                                           \n",
    "    m[\"Coverage >= 100\"] = m.groupby([\"samplename\"]).Coverage.transform(\"mean\")                                                 \n",
    "    m1 = m[[\"samplename\", \"Coverage >= 100\"]].groupby(\"samplename\").first().reset_index()                                       \n",
    "    df = df.merge(m1, on=\"samplename\", how=\"left\")                                                                                \n",
    "    df.loc[df[\"Coverage\"] < 0.10*(df[\"Coverage >= 100\"]), \"true_variant\"] = \"Low base coverage\"                                    \n",
    "    df.loc[df[\"Phred QUAL Score\"] < 200, \"true_variant\"] = \"Low Phred Score\"                                                      \n",
    "    df.loc[df[\"P-Value\"] > 0.01, \"true_variant\"] = \"Above p-value\"                                                                \n",
    "    df[\"true_variant\"] = df[\"true_variant\"].fillna(\"PASS\")\n",
    "\n",
    "    return df\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ab86b-069b-416c-8a64-d6080a9c5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for recue filtering of NO CALL variants\n",
    "# This filtering is based upon observations from Vestergaard et al., 2021. doi: 10.3390/cancers13205230.\n",
    "\n",
    "def NOCALL_recue(df, Allele_ratio, P_value, Phredscore):\n",
    "    NOCALL = df[df[\"Type\"] == \"NOCALL\"]\n",
    "    NOCALL[\"Allele_Ratio1\"] = NOCALL[\"Allele Ratio\"].str.split(\",\").str[0].str.split(\"=\").str[1].astype(\"float\")\n",
    "    NOCALL[\"Allele_Ratio2\"] = NOCALL[\"Allele Ratio\"].str.split(\",\").str[1].str.split(\"=\").str[1].astype(\"float\")\n",
    "    NOCALL.loc[NOCALL[\"Allele_Ratio2\"] == 1, \"Rescue_filtering\"] = \"Germline/NoVariant\"\n",
    "    NOCALL.loc[NOCALL[\"Allele_Ratio2\"] < Allele_ratio, \"Rescue_filtering\"] = \"Allele ratio below %f\" % (Allele_ratio)\n",
    "    NOCALL.loc[NOCALL[\"P-Value\"] > P_value, \"Rescue_filtering\"] = \"P-value above %f\" % (P_value)\n",
    "    NOCALL.loc[NOCALL[\"Phred QUAL Score\"] < Phredscore, \"Rescue_filtering\"] = \"Phred score under criteria\"\n",
    "    NOCALL.loc[NOCALL[\"UCSC Common SNPs\"] == \"YES\", \"Rescue_filtering\"] = \"Common SNP\"\n",
    "    NOCALL.loc[NOCALL[\"Homopolymer Length\"] >= 5, \"Rescue_filtering\"] = \"High Homopolymer content\"\n",
    "    NOCALL.loc[NOCALL[\"Rescue_filtering\"].isnull(), \"Rescue_filtering\"] = \"PASS NOCALL\"\n",
    "    NOCALL = NOCALL.loc[NOCALL[\"Rescue_filtering\"] == \"PASS NOCALL\"]\n",
    "    print(NOCALL[\"Rescue_filtering\"].value_counts())\n",
    "    \n",
    "    return NOCALL\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d938f0-63a2-4c36-bc95-61b3ea034384",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for rescue variant cleaning\n",
    "\n",
    "def Variant_rescue_cleaning(df):\n",
    "    df.loc[df[\"UCSC Common SNPs\"] == \"YES\", \"Rescue_cleaning\"] = \"Common SNP\"\n",
    "    df.loc[df[\"Variant Effect\"] == \"synonymous\", \"Rescue_cleaning\"] = \"synonymous mutation\" \n",
    "    df.loc[df[\"P-Value\"].isnull(), \"Rescue_cleaning\"] = \"missing p_value\"\n",
    "    df.loc[df[\"Homopolymer Length\"] >= 5, \"Rescue_cleaning\"] = \"High Homopolymer content\"\n",
    "    df.loc[df[\"Rescue_cleaning\"].isnull(), \"Rescue_cleaning\"] = \"PASS Rescue cleaning\"\n",
    "    df = df.loc[df[\"Rescue_cleaning\"] == \"PASS Rescue cleaning\"]\n",
    "\n",
    "    return df\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16fd9a-0de5-44fc-b730-28c4fd3db219",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for rescue filtering\n",
    "\n",
    "def Variant_rescue_filtering(df, Frequency, Pvalue, PhredScore, Coverage, suffix):\n",
    "    df.loc[df[\"Allele Frequency %\"] < Frequency, \"Rescue_filtering\"] = \"Allele Ratio below 2.5%\"\n",
    "    df.loc[df[\"P-Value\"] > Pvalue, \"Rescue_filtering\"] = \"P-value above criteria\"\n",
    "    df.loc[df[\"Phred QUAL Score\"] < PhredScore, \"Rescue_filtering\"] = \"Phred Score below criteria\"\n",
    "    df.loc[df[\"Raw Coverage\"] < Coverage, \"Rescue_filtering\"] = \"Coverage below criteria\"\n",
    "    df.loc[df[\"StrandBias\"] == \"Caution to strandbias\", \"Rescue_filtering\"] = \"Strandbias\"\n",
    "    df.loc[df[\"Rescue_filtering\"].isnull(), \"Rescue_filtering\"] = \"PASS Rescue filtering %s\" % (suffix)\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df301b1-861d-46a9-9fd7-c7addf57f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for calculating strand bias of identified variants\n",
    "# https://datatofish.com/if-condition-in-pandas-dataframe/    \n",
    "\n",
    "def apply_fishers_test_(df, strandbias_threshold):\n",
    "    lst_ref_var = list(df[\"Ref+/Ref-/Var+/Var-\"].str.findall(r'\\d+'))\n",
    "    df[[\"Ref+\", \"Ref-\", \"Var+\", \"Var-\"]] = lst_ref_var\n",
    "    \n",
    "    df[\"Oddsratio\"], df[\"FisherE_Pvalue\"] = zip(*df.apply(lambda r: stats.fisher_exact([[r[\"Ref+\"], \n",
    "                                                                                         r[\"Ref-\"]],\n",
    "                                                                                        [r[\"Var+\"], \n",
    "                                                                                         r[\"Var-\"]]]), axis=1))\n",
    "    \n",
    "    df[\"StrandBias\"] = df[\"FisherE_Pvalue\"].apply(lambda x: \"Caution to strandbias\" if x <= strandbias_threshold else \"No strandbias\") \n",
    "\n",
    "    return lst_ref_var\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1d7c4-e1b6-4d1f-bef7-9b6326967212",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining list of previous identified and known artificial variants\n",
    "\n",
    "artifactual_var = [[\"chr3:189456566\", \"TP63\", \"p.?\", \"Variant called on the very end of amplicon\"],\n",
    "                   [\"chr12:133220098\", \"POLE\", \"p.Val1446GlyfsTer3\", \"\"],\n",
    "                   [\"chr1:27100181\", \"ARID1A\", \"p.Gln1334del\", \"Variant called within highly repeated area of GCAGCAGCA\"],\n",
    "                   [\"chr7:87258149\", \"ABCB1, RUNDC3B\", \"p.?, p.Arg4Trp\", \"Variant called within the first three nucleotides of the amplicon\"],\n",
    "                   [\"chrX:44928852\", \"KDM6A\", \"p.Ser651Leu\", \"Variant identified from not full lenght amplicon\"],\n",
    "                   [\"chrX:44928914\", \"KDM6A\", \"p.Ala672Ser\", \"Variant identified from not full lenght amplicon\"],\n",
    "                   [\"chrX:44928920\", \"KDM6A\", \"p.Ser674Thr\", \"Variant identified from not full lenght amplicon\"],\n",
    "                   [\"chr3:10183605\", \"VHL\", \"p.Pro25Leu\", \"Variant identified from not full lenght amplicon\"],\n",
    "                   [\"chr7:151859709\", \"KMT2C\", \"p.Glu3651Asp\", \"Variant identified from not full lenght amplicon\"],\n",
    "                   [\"chr8:128750685\", \"MYC\", \"p.[Pro74=;Pro75Ser]\", \"Variant identified from not full lenght amplicon\"],\n",
    "                   [\"chr12:133212583\", \"POLE\", \"p.Ser1902PhefsTer3\", \"Variant only annotated in one direction\"],\n",
    "                   [\"chr5:79950735\", \"DHFR,MSH3\", \"p.?, p.Ala61_Pro63dup\", \"Variant identified from not full lenght amplicon and only in one direction\"], \n",
    "                   [\"chr2:48026247\", \"MSH6\", \"p.Arg379Ter\", \"Variant called within repeated area\"], \n",
    "                   [\"chr1:120612040\", \"NOTCH2\", \"p.?, p.?\", \"Variant only in one direction\"], \n",
    "                   [\"chr1:120612037\", \"NOTCH2\", \"p.?, p.?\", \"Variant only in one direction\"],\n",
    "                   [\"chr1:120612031\", \"NOTCH2\", \"p.?, p.?\", \"Variant only in one direction\"],\n",
    "                   [\"chr1:120612039\", \"NOTCH2\", \"p.?, p.?\", \"Variant only in one direction\"]]\n",
    "\n",
    "df_artifactual_var = pd.DataFrame(artifactual_var, columns =[\"Locus\", \"Gene\", \"Amino_Acid_Change\", \"Verdict\"])\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6189be-1865-4a7b-b6fb-98804dce3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to filter out artificial variants\n",
    "\n",
    "def remove_artifactual_variants(df):\n",
    "    global df_artifactual_var\n",
    "    \n",
    "    df.loc[(df[\"Locus\"].isin(df_artifactual_var[\"Locus\"])) & \n",
    "           (df[\"Amino Acid Change\"].isin(df_artifactual_var[\"Amino_Acid_Change\"])),\n",
    "           \"Common SNP\"] = \"Not common SNP, artifactual variant\"\n",
    "    \n",
    "    df.loc[df[\"Common SNP\"].isnull(), \"Common SNP\"] = \"Variant still of importance\"\n",
    "    \n",
    "    df = df.loc[df[\"Common SNP\"] == \"Variant still of importance\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "list_benigns = [\"Benign\", \"Benign/Likely benign\", \"Likely benign\"]\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa84db-408c-4058-931f-c0192a6e94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for calculating Pearson Correlation\n",
    "\n",
    "def pearson_coeff(df, input1, input2):\n",
    "    pearson_1 = set(df.loc[df[\"biopsy_type_\"] == input1][\"for_venn\"])\n",
    "    pearson_2 = set(df.loc[df[\"biopsy_type_\"] == input2][\"for_venn\"])\n",
    "    \n",
    "    pearson_int = pearson_1.intersection(pearson_2)\n",
    "    \n",
    "    pearson_1 = df.loc[(df[\"biopsy_type_\"] == input1) & (df[\"for_venn\"].isin(pearson_int))][[\"for_venn\", \"Allele Frequency %\"]]\n",
    "    pearson_1.rename(columns={\"Allele Frequency %\" : \"AF_{}\".format(input1)}, inplace=True)\n",
    "    \n",
    "    pearson_2 = df.loc[(df[\"biopsy_type_\"] == input2) & (df[\"for_venn\"].isin(pearson_int))][[\"for_venn\", \"Allele Frequency %\"]]\n",
    "    pearson_2.rename(columns={\"Allele Frequency %\" : \"AF_{}\".format(input2)}, inplace=True)\n",
    "    \n",
    "    pearson_merged = pd.merge(pearson_2, pearson_1, on=\"for_venn\")\n",
    "    \n",
    "    r, p = stats.pearsonr(pearson_merged[\"AF_{}\".format(input2)], pearson_merged[\"AF_{}\".format(input1)])\n",
    "    r = round(r, 4)\n",
    "    p = round(p, 4)\n",
    "    \n",
    "    return r, p, pearson_merged\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50bf09-ea16-421f-a500-491b18b75fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for NLP preparation of data\n",
    "\n",
    "def prep_nlp(df, diff_samples):\n",
    "    \n",
    "    dict_mutation = {\"Samplename\" : [],\n",
    "                     \"Mutation\" : []}\n",
    "    \n",
    "    for i in df[\"samplename\"].unique():\n",
    "        df_mutation = df.loc[df[\"samplename\"] == i]\n",
    "        \n",
    "        l = [', '.join(df_mutation[\"mutation\"])]\n",
    "        \n",
    "        dict_mutation[\"Samplename\"].append(i)\n",
    "        dict_mutation[\"Mutation\"].append(l)\n",
    "        \n",
    "    # Create Dataframe from dict\n",
    "    df_nlp = pd.DataFrame.from_dict(dict_mutation)\n",
    "    \n",
    "    for i in diff_samples:\n",
    "        df_nlp = df_nlp.append({\"Samplename\" : i, \"Mutation\": []}, ignore_index=True)\n",
    "        \n",
    "    df_nlp[\"Mutation\"] = df_nlp[\"Mutation\"].apply(lambda x: str(x).replace(\"[\", \"\"))\n",
    "    df_nlp[\"Mutation\"] = df_nlp[\"Mutation\"].apply(lambda x: str(x).replace(\"]\", \"\"))\n",
    "    df_nlp[\"Mutation\"] = df_nlp[\"Mutation\"].apply(lambda x: str(x).replace(\".\", \"_\"))\n",
    "    df_nlp[\"Mutation\"] = df_nlp[\"Mutation\"].apply(lambda x: str(x).replace(\":\", \"_\"))\n",
    "    df_nlp[\"Mutation\"] = df_nlp.Mutation.astype(\"str\")\n",
    "    df_nlp[\"Mutation\"] = df_nlp[\"Mutation\"].apply(lambda x: str(x).replace(\"'\", \"\"))\n",
    "    df_nlp[\"Samplename\"] = df_nlp.Samplename.apply(lambda x: x.replace(\".\", \"_\"))\n",
    "    df_nlp[\"Samplename\"] = df_nlp.Samplename.apply(lambda x: x.replace(\"_\", \"\"))\n",
    "    df_nlp[\"Samplename\"] = df_nlp.Samplename.apply(lambda x: x[-5:])\n",
    "    df_nlp[\"Samplename\"] = [x.replace(\"006\", \"T\") if x.startswith(\"006\") else x.replace(\"005\", \"A\") for x in df_nlp.Samplename]\n",
    "\n",
    "    return df_nlp\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd1f2c-4fe1-4e12-b261-73c2855ca55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for saving variant sample files seperately\n",
    "\n",
    "def saving_mut_files(df, suffix):\n",
    "\n",
    "    for i,j in zip(df.Samplename, df.Mutation):\n",
    "\n",
    "        file_name = i+suffix+\".txt\"\n",
    "    \n",
    "        text_file = open(file_name, \"w\")\n",
    "        text_file.write(j)\n",
    "        text_file.close()\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb87db6-c912-464d-a708-5052c9226cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for calculating Cosine Similarity\n",
    "\n",
    "def Cosine_similarity(corpus):\n",
    "    \n",
    "    #Instantiate a count vector\n",
    "    count_vect = CountVectorizer()\n",
    "  \n",
    "    # Train the Count Vectorizer.\n",
    "    X_train_counts = count_vect.fit_transform(corpus)\n",
    "    \n",
    "    # Converting the X_train_counts to a dataframe. \n",
    "    df_doc = pd.DataFrame(X_train_counts.toarray(), columns=count_vect.get_feature_names())\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    co_sim = cosine_similarity(X_train_counts, X_train_counts)\n",
    "    \n",
    "    # Instantiate a TfidfVectorizer\n",
    "    vectorizer_tfidf = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the data    \n",
    "    trsfm = trsfm = vectorizer_tfidf.fit_transform(corpus)\n",
    "    \n",
    "    # Converting trsfm to a dataframe\n",
    "    df_doc1 = pd.DataFrame(trsfm.toarray(), columns=count_vect.get_feature_names()) # Make principle component analysis on this. \n",
    "\n",
    "    # Calculating cosine similarity\n",
    "    co_sim1 = cosine_similarity(trsfm, trsfm)\n",
    "\n",
    "    # Creating a dataframe over the cosine similarities\n",
    "    co_sim1_df = pd.DataFrame(co_sim1)\n",
    "    \n",
    "    return co_sim1_df, df_doc1\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986931ba-9798-4385-8e54-bd93e795ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for plotting Clustermap\n",
    "\n",
    "def plotting_clustermap(df):\n",
    "\n",
    "    plt.figure()\n",
    "    sns.set_theme(style=\"white\")\n",
    "    g = sns.clustermap(df, yticklabels=True, xticklabels=True,\n",
    "                       figsize=(35, 35), cmap=\"vlag\", annot=False)\n",
    "\n",
    "    mask = np.tril(np.ones_like(df))\n",
    "    mask_inverted = np.logical_not(mask).astype(int)\n",
    "\n",
    "    values = g.ax_heatmap.collections[0].get_array().reshape(df.shape)\n",
    "    new_values = np.ma.array(values, mask=mask)\n",
    "    new_values_inverted = np.ma.array(values, mask=mask)\n",
    "\n",
    "    g.ax_heatmap.collections[0].set_array(new_values)\n",
    "    g.ax_row_dendrogram.set_visible(False)\n",
    "    \n",
    "    plt.setp(g.ax_heatmap.get_yticklabels(), fontsize=25)\n",
    "    plt.setp(g.ax_heatmap.get_xticklabels(), fontsize=25)\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff859679-3d24-4344-bb39-2e633e90ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for plotting networks\n",
    "# https://networkx.org/documentation/stable/auto_examples/drawing/plot_weighted_graph.html\n",
    "\n",
    "def plot_network(sources, targets, weights):\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    # Create directed graph object\n",
    "    Graph = nx.Graph()\n",
    "\n",
    "    for s,t,w in zip(sources, targets, weights.round(2)):\n",
    "        Graph.add_edge(s, t, weight=w, value=w)\n",
    "\n",
    "    exlarge = [(u, v) for (u, v, d) in Graph.edges(data=True) if d[\"weight\"] > 0.90]\n",
    "    elarge = [(u, v) for (u, v, d) in Graph.edges(data=True) if d[\"weight\"] > 0.75 and d[\"weight\"] < 0.90]\n",
    "    emedium = [(u, v) for (u, v, d) in Graph.edges(data=True) if d[\"weight\"] > 0.50 and d[\"weight\"] < 0.75]\n",
    "    esmall = [(u, v) for (u, v, d) in Graph.edges(data=True) if d[\"weight\"] < 0.50]\n",
    "\n",
    "    pos = nx.spring_layout(Graph, seed=17, dim=2, iterations=40)\n",
    "    nx.draw_networkx_edges(Graph, pos, edgelist=esmall, width=2, alpha=0.05, edge_color=\"gray\", style=\"dashed\")\n",
    "    nx.draw_networkx_edges(Graph, pos, edgelist=exlarge, width=10, alpha=0.7, edge_color=\"#9F0E14\", style=\"solid\")\n",
    "    nx.draw_networkx_edges(Graph, pos, edgelist=elarge, width=8, alpha=0.7, edge_color=\"#E43027\", style=\"solid\")\n",
    "    nx.draw_networkx_edges(Graph, pos, edgelist=emedium, width=4, alpha=0.7, edge_color=\"#FB7050\", style=\"solid\")\n",
    "\n",
    "    # node labels\n",
    "    nx.draw_networkx_labels(Graph, pos, font_size=20, font_family=\"sans-serif\")\n",
    "\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.margins(0.08)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7e000-2d3b-4408-a963-9e673736fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for plotting source targets weights in network\n",
    "\n",
    "def source_targets_weights(df):\n",
    "    \n",
    "    # Getting all combinations of cosine similarities matrix in two dimensions\n",
    "    t = df.stack()\n",
    "    t = t.reset_index()\n",
    "    t = t.rename(columns={\"level_0\": \"node\", \"level_1\": \"connection\", 0 : \"cosine_weight\"})\n",
    "    \n",
    "    # Remove samples that have the same node and connection, as they are the sample itself.\n",
    "    t = t[t[\"node\"] != t[\"connection\"]]\n",
    "    # t = t.drop(t[t[\"cosine_weight\"] == 0].index)\n",
    "    \n",
    "    # Saving object into sources, targets and weights. \n",
    "    sources = t[\"node\"]\n",
    "    targets = t[\"connection\"]\n",
    "    weights = t[\"cosine_weight\"]\n",
    "\n",
    "    return sources, targets, weights\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a820e-5311-4696-9dd5-9ad9978a6c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
